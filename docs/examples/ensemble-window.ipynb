{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHSd310cTJGp"
   },
   "source": [
    "# Changepoint Detection Ensembles: Window search method\n",
    "\n",
    "<!-- {{ add_binder_block(page) }} -->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Many changepoint detection (CPD) procedure can be implemented with `ruptures`. In most case, these procedures involve a search method relying on only one cost function. We are investigating here a way of building an ensemble model, on top of `ruptures` package, using the [window search method](../user-guide/detection/window.md) that will rely on several cost functions. The incentive for this approach is to combine the methods considering only one cost into an aggregated score. \n",
    "\n",
    "The interested reader can refer to [Katser2021](#Katser2021) for a more complete introduction. As in the paper, we will make the assumption that the number of changepoints to detect is known in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfsRv7R7TTpW"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, we make the necessary imports and generate a toy signal. In order to highlight the strength of the ensemble model, we concatenate two kinds of signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "xrQQA7izTVnG",
    "outputId": "42071353-5469-4665-dda0-9e53f99ec5c4"
   },
   "outputs": [],
   "source": [
    "import time  # for execution time comparison\n",
    "\n",
    "import matplotlib.pyplot as plt  # for display purposes\n",
    "import numpy as np\n",
    "\n",
    "import ruptures as rpt  # our package\n",
    "from ruptures.metrics import randindex\n",
    "\n",
    "# generate a signal\n",
    "n_samples, dim, sigma = 250, 3, 4\n",
    "n_bkps = 10  # number of breakpoints\n",
    "\n",
    "# to make it more complex, we concatenate two different signals together\n",
    "signal_constant, bkps_constant = rpt.pw_constant(\n",
    "    n_samples=n_samples, n_features=dim, n_bkps=n_bkps // 2, noise_std=sigma, seed=1\n",
    ")\n",
    "signal_linear, bkps_linear = rpt.pw_linear(\n",
    "    n_samples=n_samples,\n",
    "    n_features=dim - 1,\n",
    "    n_bkps=n_bkps - n_bkps // 2 - 1,\n",
    "    noise_std=sigma,\n",
    "    seed=1,\n",
    ")\n",
    "signal = np.append(signal_constant, signal_linear, axis=0)\n",
    "bkps = sorted(bkps_constant + list(np.array(bkps_linear) + n_samples))\n",
    "\n",
    "# As it is done in the paper, we use the z-normalization\n",
    "signal = (signal - signal.mean(axis=0)) / signal.std(axis=0)\n",
    "\n",
    "fig, ax_array = rpt.display(signal, bkps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23czz_5OXsQM"
   },
   "source": [
    "## Window search method\n",
    "\n",
    "In the following, we are going to use the cost functions:\n",
    "\n",
    "- [Autoregressive model change](../user-guide/costs/costautoregressive.md) (ar)\n",
    "- [Change detection with a Mahalanobis-type metric](../user-guide/costs/costml.md) (mahalanobis)\n",
    "- [Least absolute deviation](../user-guide/costs/costl1.md) (l1)\n",
    "- [Least squared deviation](../user-guide/costs/costl2.md) (l2)\n",
    "- [Linear model change](../user-guide/costs/costlinear.md) (linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouDY8rL_j2GK"
   },
   "outputs": [],
   "source": [
    "cost_functions = [\n",
    "    \"ar\",\n",
    "    \"mahalanobis\",\n",
    "    \"l1\",\n",
    "    \"l2\",\n",
    "    \"linear\",\n",
    "]  # cost function that we will use\n",
    "window_size = 20  # best hyperparameter according to the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xgYOobTXxAH"
   },
   "source": [
    "### Classical method: one cost for one prediction\n",
    "\n",
    "Let's start with the classical method: the window search method rely on only one cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qHnV7U_ZVhI",
    "outputId": "f89ee568-6edc-4530-8f2b-58d9b96ee14a"
   },
   "outputs": [],
   "source": [
    "scores = []  # save the scores for the ensemble method (see next section)\n",
    "rand_indexes = {}  # to keep track of the performances\n",
    "\n",
    "for cost in cost_functions:\n",
    "    start_time = time.time()\n",
    "\n",
    "    algo = rpt.Window(width=window_size, model=cost, jump=1).fit(signal)\n",
    "    predicted_bkps = algo.predict(n_bkps=n_bkps)\n",
    "\n",
    "    print(f\"{cost} cost computation in {time.time() - start_time:.3f} s\")\n",
    "\n",
    "    rand_indexes[cost] = np.around(randindex(bkps, predicted_bkps), 3)\n",
    "    scores.append(algo.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the performances of the classical method on each cost function, let's dive into the ensemble method. We will then compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qB3Nhz3njORl"
   },
   "source": [
    "### Ensemble method: multiple costs for one prediction\n",
    "\n",
    "The paper we rely on [Katser2021](#Katser2021), uses the score from each cost function to detect the changepoints. This is why we stored them in the previous section."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!!! note\n",
    "    As a reminder, [here](../user-guide/detection/window.md) is how the score is computed for the window-based search method, it can also be called the *discrepancy measure*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how those scores look like. In the following cell, each row represents the score for a specific cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "nBk9sihcLk2d",
    "outputId": "90bf9753-3ca1-43fd-a3c9-d103c5a923a0"
   },
   "outputs": [],
   "source": [
    "scores = np.array(scores).T\n",
    "\n",
    "# Display the scores with the change points to predict\n",
    "append_scores = np.append(\n",
    "    np.ones((window_size // 2, len(cost_functions))) * float(\"inf\"), scores, axis=0\n",
    ")\n",
    "_ = rpt.display(append_scores, bkps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the CPD Ensemble method is to scale and aggregate the scores with some well chosen scaling and aggregation function to get new score which will allow us to get a better prediction of the changepoints.\n",
    "\n",
    "In the paper, they perform multiple tests on several scaling and aggregation functions on two datasets. In turned out that the *MinAbs* scaling function and the *WeightedSum* aggregation function worked the best.\n",
    "\n",
    "The *MinAbs* function is defined as follows:\n",
    "For $s$ a timeseries, \n",
    "\n",
    "$$\n",
    "\\textit{MinAbs}(s)_i = \\frac{s_i}{|\\min_{j}{s_j}|}\n",
    "$$\n",
    "\n",
    "The *WeightedSum* function is defined as follows:\n",
    "Let $s^n, n \\in \\{1, ..., N\\}$ be $N$ timeseries, we have to distinguish the \"original\" timeseries $s^n$ from its scaled version $\\overline{s}^n = \\textit{MinAbs}(s^n)$. We then have\n",
    "\n",
    "$$ \\textit{WeightedSum}((s_n)_{n \\in \\{1, ..., N\\}})_i = \\sum_{n \\in \\{1, ..., N\\}}{\\lambda_n \\overline{s}_i^n} \n",
    "$$\n",
    "\n",
    "where $\\lambda_n = \\frac{\\max_{j}{s_j^n} - \\min_{j}{s_j^n}}{\\mu(s^n) - \\min_{j}{s_j^n}}$ with $\\mu(s^n)$ the mean of $s^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "qEGZDKM4PyzP",
    "outputId": "7b196571-3ad8-4c51-c386-5fc7af29000c"
   },
   "outputs": [],
   "source": [
    "def min_abs_scaling(array):\n",
    "    return array / abs(np.min(array, axis=0) + 1e-8)\n",
    "\n",
    "\n",
    "def weighted_sum_aggregation(array):\n",
    "    min_array = array.min(axis=0)\n",
    "    weights = (array.max(axis=0) - min_array) / (array.mean(axis=0) - min_array)\n",
    "\n",
    "    return min_abs_scaling(array) @ weights\n",
    "\n",
    "\n",
    "aggregated_scores = weighted_sum_aggregation(scores)\n",
    "\n",
    "# Display scaled and aggregated scores\n",
    "append_scaled_aggregated_scores = np.append(\n",
    "    np.ones(window_size // 2) * float(\"inf\"), aggregated_scores\n",
    ")\n",
    "rpt.display(append_scaled_aggregated_scores, bkps)\n",
    "_ = plt.title(\"Scaled and aggregated score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now detect the changepoints from the newly computed score. For that we need to define a *DummyCost* that will allow us to leverage `ruptures` power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4u4Hx1vtWdw-"
   },
   "outputs": [],
   "source": [
    "from ruptures.base import BaseCost\n",
    "\n",
    "\n",
    "class DummyCost(BaseCost):\n",
    "\n",
    "    r\"\"\"\n",
    "    Dummy cost to pretend a real cost function.\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"Dummy\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the object.\"\"\"\n",
    "        self.signal = None\n",
    "\n",
    "    def fit(self, signal) -> \"DummyCost\":\n",
    "        \"\"\"Set parameters of the instance.\n",
    "        Args:\n",
    "            signal (array): signal. Shape (n_samples,) or (n_samples, n_features)\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        if signal.ndim == 1:\n",
    "            self.signal = signal.reshape(-1, 1)\n",
    "        else:\n",
    "            self.signal = signal\n",
    "\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end) -> float:\n",
    "        \"\"\"Return the approximation cost on the segment [start:end].\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "        Returns:\n",
    "            + infinity\n",
    "        \"\"\"\n",
    "        return float(\"inf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are using the window search method to predict the changepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "Ly9e_TLpQ_PB",
    "outputId": "99f3d70c-3f6b-47f9-fda5-fe544f385e6e"
   },
   "outputs": [],
   "source": [
    "# create the ensemble change point detector\n",
    "dummy_cost = DummyCost().fit(signal)\n",
    "algo = rpt.Window(width=window_size, custom_cost=dummy_cost, jump=1)\n",
    "algo.fit(signal)\n",
    "algo.score = aggregated_scores\n",
    "\n",
    "ensemble_predicted_bkps = algo.predict(n_bkps=n_bkps)\n",
    "rand_indexes[\"ensemble\"] = np.around(randindex(bkps, ensemble_predicted_bkps), 3)\n",
    "\n",
    "_ = [print(cost, rand_index) for cost, rand_index in rand_indexes.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "8bubkcRYQzWf",
    "outputId": "195c0ee5-9f5f-4f17-f3ac-c27a51de6701"
   },
   "outputs": [],
   "source": [
    "_ = rpt.display(signal, bkps, ensemble_predicted_bkps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this example, we have seen how to build an ensemble model to detect changepoints in a few lines of code.\n",
    "\n",
    "This example is using a window search method, the algorithm for ensemble models using other search methods like Binary segmentation or Dynamic programming are given in the paper [Katser2021](#Katser2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "This example notebook has been authored by [Th√©o VINCENT](https://github.com/theovincent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3JICZPZZ5dU"
   },
   "source": [
    "### References\n",
    "\n",
    "<a id=\"Katser2021\">[Katser2021]</a>\n",
    "Katser, I., Kozitsin, V., Lobachev, V., & Maksimov, I. (2021). Unsupervised Offline Changepoint Detection Ensembles. Applied Sciences, 11(9), 4280."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1ufL03Ine4kl",
    "lFfnkl2rTAsy",
    "uozXK6DbnC5p",
    "eOIc_86NKLBi",
    "Vmhms6jkMjH4"
   ],
   "name": "CPDE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
