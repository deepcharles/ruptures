{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHSd310cTJGp"
   },
   "source": [
    "# Combining several cost functions\n",
    "\n",
    "<!-- {{ add_binder_block(page) }} -->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In `ruptures`, change point detection procedures make use of only one cost function.\n",
    "The choice of the cost function is critical as it is related to the type of change to find. \n",
    "For instance, [CostL2](../user-guide/costs/costl2.md) can detect shifts in the mean, [CostNormal](../user-guide/costs/costnormal.md) can detect shifts in the mean and the covariance structure, [CostAR](../user-guide/costs/costautoregressive.md) can detect shifts in the auto-regressive structure, etc.\n",
    "\n",
    "However, in many settings, several types of changes co-exist in the same signal and a single cost function is not able to spot all changes simultaneously.\n",
    "To cope with this issue, a procedure to merge several cost functions has been introduced [[Katser2021]](#Katser2021).\n",
    "In a nutshell, a number of costs can be combined to yield an aggregated cost function which is sensitive to several types of changes.\n",
    "The aggregated cost can then be used with any search method (such as the [window search method](../user-guide/detection/window.md)) to create change point detection algorithm.\n",
    "\n",
    "This example illustrates the aggregation procedure, also referred to as an ensemble model.\n",
    "Here, only [dynamic programming](../user-guide/detection/dynp.md) is considered, but all other search methods (e.g. [window sliding search method](../user-guide/detection/window.md), [PELT](../user-guide/detection/pelt.md), [binary segmentation](../user-guide/detection/binseg.md)) could be used.\n",
    "In addition, the number of changes is assumed to be known by the user.\n",
    "More details can be found in the original paper introducing the cost aggregation procedure [[Katser2021]](#Katser2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfsRv7R7TTpW"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, we make the necessary imports and generate a multivariate toy signal which contains mean shifts and linear changes (i.e. changes in the linear relationship between the dimensions of the signal). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "xrQQA7izTVnG",
    "outputId": "42071353-5469-4665-dda0-9e53f99ec5c4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # for display purposes\n",
    "import numpy as np\n",
    "\n",
    "import ruptures as rpt  # our package\n",
    "from ruptures.metrics import hamming\n",
    "\n",
    "# generate a signal\n",
    "n_samples, n_dims, sigma = 500, 3, 4\n",
    "n_bkps = 10  # number of breakpoints\n",
    "\n",
    "# to make it more complex, we concatenate two different signals together\n",
    "signal_constant, bkps_constant = rpt.pw_constant(\n",
    "    n_samples=n_samples // 2, n_features=n_dims, n_bkps=n_bkps // 2, noise_std=sigma\n",
    ")\n",
    "\n",
    "signal_linear, bkps_linear = rpt.pw_linear(\n",
    "    n_samples=n_samples // 2,\n",
    "    n_features=n_dims - 1,\n",
    "    n_bkps=n_bkps - n_bkps // 2 - 1,\n",
    "    noise_std=0,\n",
    ")\n",
    "\n",
    "signal = np.r_[signal_constant, signal_linear]\n",
    "bkps_true = sorted(bkps_constant + list(np.array(bkps_linear) + n_samples // 2))\n",
    "\n",
    "# z-normalization\n",
    "signal = (signal - signal.mean(axis=0)) / signal.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the signal and the true change points (changes occur when the background colour shifts from blue to pink and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the signal\n",
    "fig, ax_array = rpt.display(signal, bkps_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one cost function at a time\n",
    "\n",
    "Recall that two types of changes are present in the previous signal: mean shifts and linear changes.\n",
    "The most adapted costs for these types are:\n",
    "\n",
    "- [CostL2](../user-guide/costs/costl2.md) (for mean shifts),\n",
    "- [CostLinear](../user-guide/costs/costlinear.md) (for linear changes).\n",
    "\n",
    "The following cell shows that using a single cost function is not enough to detect all changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qHnV7U_ZVhI",
    "outputId": "f89ee568-6edc-4530-8f2b-58d9b96ee14a"
   },
   "outputs": [],
   "source": [
    "list_of_cost_functions = [\"l2\", \"linear\"]\n",
    "\n",
    "for cost_str in list_of_cost_functions:\n",
    "    # Compute the changes\n",
    "    algo = rpt.Dynp(model=cost_str).fit(signal)\n",
    "    predicted_bkps = algo.predict(n_bkps=n_bkps)\n",
    "    # Display the prediction\n",
    "    fig, (ax,) = rpt.display(signal[:, 0], bkps_true, predicted_bkps)\n",
    "    ax.margins(x=0)\n",
    "    ax.set_title(\n",
    "        f\"Cost {cost_str} (Hamming error: {hamming(bkps_true, predicted_bkps):.2f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous plots, only the first dimension of the signal is shown. The hamming error is also added. It goes from 0 to 1, the lower the better.\n",
    "\n",
    "The first plot shows that the [CostL2](../user-guide/costs/costl2.md) allows the [dynamic programming detector](../user-guide/detection/dynp.md) to find the true breakpoints only on the first part of the signal. Whereas the second plot tells us that the [CostLinear](../user-guide/costs/costlinear.md) is less helpful on the entire signal.\n",
    "The overall performance of the two detectors is not satisfying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qB3Nhz3njORl"
   },
   "source": [
    "## Using several cost functions: the ensemble method\n",
    "\n",
    "Roughly, the ensemble method scales each individual score and aggregates them to get a new score which will allow us to get a better prediction of the changepoints.\n",
    "\n",
    "In [[Katser2021]](#Katser2021), the authors perform multiple tests on several scaling and aggregation functions on two datasets. It turned out that the *MinMax* scaling function and the *Min* aggregation function worked best.\n",
    "\n",
    "The *MinMax* function is defined as follows:\n",
    "For $s$ a timeseries, \n",
    "\n",
    "$$\n",
    "\\textit{MinMax}(s)_i = \\frac{s_i - \\max_{j}{s_j}}{\\max_{j}{s_j} - \\min_{j}{s_j}}\n",
    "$$\n",
    "\n",
    "The *Min* function is defined as follows:\n",
    "Let $s^n, n \\in \\{1, ..., N\\}$ be $N$ timeseries, we have\n",
    "$$ \n",
    "\\textit{Min}((s^n)_{n \\in \\{1, ..., N\\}})_i = \\min_{p \\in \\{1, ..., N\\}}{s^p_i} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "qEGZDKM4PyzP",
    "outputId": "7b196571-3ad8-4c51-c386-5fc7af29000c"
   },
   "outputs": [],
   "source": [
    "def min_max_scaling(array):\n",
    "    return (array - np.max(array, axis=0)) / (\n",
    "        np.max(array, axis=0) - np.min(array, axis=0) + 1e-8\n",
    "    )\n",
    "\n",
    "\n",
    "def min_aggregation(array):\n",
    "    return array.min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new detector needs to be build in order to combine the [CostL2](../user-guide/costs/costl2.md) and the [CostLinear](../user-guide/costs/costlinear.md). Here is how it can be build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4u4Hx1vtWdw-"
   },
   "outputs": [],
   "source": [
    "r\"\"\"Dynamic programming\"\"\"\n",
    "from functools import lru_cache\n",
    "\n",
    "from ruptures.utils import sanity_check\n",
    "from ruptures.costs import cost_factory\n",
    "from ruptures.exceptions import BadSegmentationParameters\n",
    "\n",
    "\n",
    "class DynpEnsemble(rpt.Dynp):\n",
    "\n",
    "    \"\"\"Find optimal change points using dynamic programming with more than one cost.\n",
    "\n",
    "    Given a segment model, it computes the best partition for which the\n",
    "    sum of errors is minimum.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models, min_size=2, jump=5, params=None):\n",
    "        \"\"\"Creates a Dynp instance.\n",
    "\n",
    "        Args:\n",
    "            models (list[str], optional): segment model, ex: [\"l1\"], [\"l2\", \"rbf\"], ...\n",
    "            min_size (int, optional): minimum segment length.\n",
    "            jump (int, optional): subsample (one every *jump* points).\n",
    "            params (dict, optional): a dictionary of dictionnaries of parameters for the cost instances. One dictionnary of parameters per cost instance.\n",
    "        \"\"\"\n",
    "        self.model_names = models\n",
    "        self.costs = []\n",
    "        self.min_size = min_size\n",
    "        for model in models:\n",
    "            if params is None:\n",
    "                self.costs.append(cost_factory(model=model))\n",
    "            else:\n",
    "                self.costs.append(cost_factory(model=model, **params[model]))\n",
    "            self.min_size = max(self.min_size, self.costs[-1].min_size)\n",
    "        self.jump = jump\n",
    "        self.n_samples = None\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def seg(self, start, end, n_bkps):\n",
    "        \"\"\"Recurrence to find the optimal partition of signal[start:end].\n",
    "\n",
    "        This method is to be memoized and then used.\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment (inclusive)\n",
    "            end (int): end of the segment (exclusive)\n",
    "            n_bkps (int): number of breakpoints\n",
    "\n",
    "        Returns:\n",
    "            dict: {(start, end): cost value, ...}\n",
    "        \"\"\"\n",
    "        jump, min_size = self.jump, self.min_size\n",
    "\n",
    "        if n_bkps == 0:\n",
    "            cost = [\n",
    "                self.costs[idx_cost].error(start, end)\n",
    "                for idx_cost in range(len(self.costs))\n",
    "            ]\n",
    "            return {(start, end): np.array(cost)}\n",
    "        elif n_bkps > 0:\n",
    "            # Let's fill the list of admissible last breakpoints\n",
    "            multiple_of_jump = (k for k in range(start, end) if k % jump == 0)\n",
    "            admissible_bkps = list()\n",
    "            for bkp in multiple_of_jump:\n",
    "                n_samples = bkp - start\n",
    "                # first check if left subproblem is possible\n",
    "                if sanity_check(\n",
    "                    n_samples=n_samples,\n",
    "                    n_bkps=n_bkps - 1,\n",
    "                    jump=jump,\n",
    "                    min_size=min_size,\n",
    "                ):\n",
    "                    # second check if the right subproblem has enough points\n",
    "                    if end - bkp >= min_size:\n",
    "                        admissible_bkps.append(bkp)\n",
    "\n",
    "            assert (\n",
    "                len(admissible_bkps) > 0\n",
    "            ), \"No admissible last breakpoints found.\\\n",
    "             start, end: ({},{}), n_bkps: {}.\".format(\n",
    "                start, end, n_bkps\n",
    "            )\n",
    "\n",
    "            # Compute the subproblems\n",
    "            sub_problems = list()\n",
    "            for bkp in admissible_bkps:\n",
    "                left_partition = self.seg(start, bkp, n_bkps - 1)\n",
    "                right_partition = self.seg(bkp, end, 0)\n",
    "                tmp_partition = dict(left_partition)\n",
    "                tmp_partition[(bkp, end)] = right_partition[(bkp, end)]\n",
    "                sub_problems.append(tmp_partition)\n",
    "\n",
    "            # compute the matrix of cost, row = sub_problem, column = model\n",
    "            cost_matrix = [\n",
    "                np.sum(list(sub_problem.values()), axis=0)\n",
    "                for sub_problem in sub_problems\n",
    "            ]\n",
    "\n",
    "            aggregated_costs = min_aggregation(min_max_scaling(np.array(cost_matrix)))\n",
    "\n",
    "            # Find the optimal partition\n",
    "            return sub_problems[np.argmin(aggregated_costs)]\n",
    "\n",
    "    def fit(self, signal) -> \"DynpEnsemble\":\n",
    "        \"\"\"Create the cache associated with the signal.\n",
    "\n",
    "        Dynamic programming is a recurrence; intermediate results are cached to speed up\n",
    "        computations. This method sets up the cache.\n",
    "\n",
    "        Args:\n",
    "            signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # clear cache\n",
    "        self.seg.cache_clear()\n",
    "        # update some params\n",
    "        [self.costs[idx_cost].fit(signal) for idx_cost in range(len(self.costs))]\n",
    "        self.n_samples = signal.shape[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, n_bkps):\n",
    "        \"\"\"Return the optimal breakpoints.\n",
    "\n",
    "        Must be called after the fit method. The breakpoints are associated with the signal passed\n",
    "        to [`fit()`][ruptures.detection.dynp.Dynp.fit].\n",
    "\n",
    "        Args:\n",
    "            n_bkps (int): number of breakpoints.\n",
    "\n",
    "        Raises:\n",
    "            BadSegmentationParameters: in case of impossible segmentation\n",
    "                configuration\n",
    "\n",
    "        Returns:\n",
    "            list: sorted list of breakpoints\n",
    "        \"\"\"\n",
    "        # raise an exception in case of impossible segmentation configuration\n",
    "        if not sanity_check(\n",
    "            n_samples=self.costs[0].signal.shape[0],\n",
    "            n_bkps=n_bkps,\n",
    "            jump=self.jump,\n",
    "            min_size=self.min_size,\n",
    "        ):\n",
    "            raise BadSegmentationParameters\n",
    "        partition = self.seg(0, self.n_samples, n_bkps)\n",
    "        bkps = sorted(e for s, e in partition.keys())\n",
    "        return bkps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly made `DynpEnsemble` class is ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "Ly9e_TLpQ_PB",
    "outputId": "99f3d70c-3f6b-47f9-fda5-fe544f385e6e"
   },
   "outputs": [],
   "source": [
    "# create the ensemble change point detector\n",
    "algo = DynpEnsemble([\"l2\", \"linear\"])\n",
    "algo.fit(signal)\n",
    "\n",
    "ensemble_predicted_bkps = algo.predict(n_bkps=n_bkps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "8bubkcRYQzWf",
    "outputId": "195c0ee5-9f5f-4f17-f3ac-c27a51de6701"
   },
   "outputs": [],
   "source": [
    "# Display the prediction\n",
    "fig, (ax,) = rpt.display(signal[:, 0], bkps_true, ensemble_predicted_bkps)\n",
    "ax.margins(x=0)\n",
    "_ = ax.set_title(\n",
    "    f\"Cost ensemble (Hamming error: {hamming(bkps_true, ensemble_predicted_bkps):.2f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this example, we have seen how to build an ensemble model to detect changepoints in a few lines of code.\n",
    "\n",
    "This example is using a dynamic programming search method, the algorithm for ensemble models using other search methods like binary segmentation or window search are given in the paper [Katser2021](#Katser2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "This example notebook has been authored by [Th√©o VINCENT](https://github.com/theovincent) and edited by [Olivier Boulant](https://github.com/oboulant) and [Charles Truong](https://github.com/deepcharles).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3JICZPZZ5dU"
   },
   "source": [
    "## References\n",
    "\n",
    "<a id=\"Katser2021\">[Katser2021]</a>\n",
    "Katser, I., Kozitsin, V., Lobachev, V., & Maksimov, I. (2021). Unsupervised Offline Changepoint Detection Ensembles. Applied Sciences, 11(9), 4280."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1ufL03Ine4kl",
    "lFfnkl2rTAsy",
    "uozXK6DbnC5p",
    "eOIc_86NKLBi",
    "Vmhms6jkMjH4"
   ],
   "name": "CPDE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
