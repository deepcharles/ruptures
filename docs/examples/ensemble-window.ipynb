{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHSd310cTJGp"
   },
   "source": [
    "# Combining several cost functions\n",
    "\n",
    "<!-- {{ add_binder_block(page) }} -->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In `ruptures`, change point detection procedures make use of only one cost function.\n",
    "The choice of the cost function is critical as it is related to the type of change to find. \n",
    "For instance, [CostL2](../user-guide/costs/costl2.md) can detect shifts in the mean, [CostNormal](../user-guide/costs/costnormal.md) can detect shifts in the mean and the covariance structure, [CostAR](../user-guide/costs/costautoregressive.md) can detect shifts in the auto-regressive structure, etc.\n",
    "\n",
    "However, in many settings, several types of changes co-exist in the same signal and a single cost function is not able to spot all changes simultaneously.\n",
    "To cope with this issue, a procedure to merge several cost functions has been introduced [[Katser2021]](#Katser2021).\n",
    "In a nutshell, a number of costs can be combined to yield an aggregated cost function which is sensitive to several types of changes.\n",
    "The aggregated cost can then be used with any search method (such as the [window search method](../user-guide/detection/window.md)) to create change point detection algorithm.\n",
    "\n",
    "This example illustrates the aggregation procedure, also referred to as an ensemble model.\n",
    "Here, only dynamic programming is considered, but all other search methods (e.g. [window sliding search method](../user-guide/detection/window.md), [PELT](../user-guide/detection/pelt.md), [binary segmentation](../user-guide/detection/binseg.md)) could be used.\n",
    "In addition, the number of changes is assumed to be known by the user.\n",
    "More details can be found in the original paper introducing the cost aggregation procedure [[Katser2021]](#Katser2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfsRv7R7TTpW"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, we make the necessary imports and generate a multivariate toy signal which contains mean shifts and linear changes (i.e. changes in the linear relationship between the dimensions of the signal). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "xrQQA7izTVnG",
    "outputId": "42071353-5469-4665-dda0-9e53f99ec5c4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # for display purposes\n",
    "import numpy as np\n",
    "\n",
    "import ruptures as rpt  # our package\n",
    "from ruptures.metrics import hamming\n",
    "\n",
    "# generate a signal\n",
    "n_samples, n_dims, sigma = 500, 3, 4\n",
    "n_bkps = 10  # number of breakpoints\n",
    "\n",
    "# to make it more complex, we concatenate two different signals together\n",
    "signal_constant, bkps_constant = rpt.pw_constant(\n",
    "    n_samples=n_samples // 2, n_features=n_dims, n_bkps=n_bkps // 2, noise_std=sigma\n",
    ")\n",
    "\n",
    "signal_linear, bkps_linear = rpt.pw_linear(\n",
    "    n_samples=n_samples // 2,\n",
    "    n_features=n_dims - 1,\n",
    "    n_bkps=n_bkps - n_bkps // 2 - 1,\n",
    "    noise_std=sigma,\n",
    ")\n",
    "\n",
    "signal = np.r_[signal_constant, signal_linear]\n",
    "bkps_true = sorted(bkps_constant + list(np.array(bkps_linear) + n_samples // 2))\n",
    "\n",
    "# z-normalization\n",
    "signal = (signal - signal.mean(axis=0)) / signal.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the signal and the true change points (changes occur when the background colour shifts from blue to pink and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the signal\n",
    "fig, ax_array = rpt.display(signal, bkps_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one cost function at a time\n",
    "\n",
    "Recall that two types of changes are present in the previous signal: mean shifts and linear changes.\n",
    "The most adapted costs for these types are:\n",
    "\n",
    "- [CostL2](../user-guide/costs/costl2.md) (for mean shifts),\n",
    "- [CostLinear](../user-guide/costs/costlinear.md) (for linear changes).\n",
    "\n",
    "The following cell shows that using a single cost function is not enough to detect all changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qHnV7U_ZVhI",
    "outputId": "f89ee568-6edc-4530-8f2b-58d9b96ee14a"
   },
   "outputs": [],
   "source": [
    "list_of_cost_functions = [\"l2\", \"linear\"]\n",
    "\n",
    "for cost_str in list_of_cost_functions:\n",
    "    # Compute the changes\n",
    "    algo = rpt.Dynp(model=cost_str).fit(signal)\n",
    "    predicted_bkps = algo.predict(n_bkps=n_bkps)\n",
    "    # Display the prediction\n",
    "    fig, (ax,) = rpt.display(signal[:, 0], bkps_true, predicted_bkps)\n",
    "    ax.margins(x=0)\n",
    "    ax.set_title(\n",
    "        f\"Cost {cost_str} (Hamming error: {hamming(bkps_true, predicted_bkps):.2f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "ADD SMALL COMMENTS \n",
    "\n",
    "    - Only the first dimension of the signal is shown\n",
    "    - the hamming error: between 0 and 1, lower is better\n",
    "    - CostL2 detects the mean shifts, but not the linear shifts\n",
    "    - CostLinear is not good on either changes.\n",
    "    - Overall performance is not good.\n",
    "</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qB3Nhz3njORl"
   },
   "source": [
    "## Using several cost functions: the ensemble method\n",
    "\n",
    "Roughly, the ensemble method scales each  and aggregate the individual scores to get a new score which will allow us to get a better prediction of the changepoints.\n",
    "\n",
    "In [[Katser2021]](#Katser2021), the authors perform multiple tests on several scaling and aggregation functions on two datasets. It turned out that the *MinAbs* scaling function and the *WeightedSum* aggregation function worked best.\n",
    "\n",
    "The *MinAbs* function is defined as follows:\n",
    "For $s$ a timeseries, \n",
    "\n",
    "$$\n",
    "\\textit{MinAbs}(s)_i = \\frac{s_i}{|\\min_{j}{s_j}|}\n",
    "$$\n",
    "\n",
    "The *WeightedSum* function is defined as follows:\n",
    "Let $s^n, n \\in \\{1, ..., N\\}$ be $N$ timeseries, we have to distinguish the \"original\" timeseries $s^n$ from its scaled version $\\overline{s}^n = \\textit{MinAbs}(s^n)$. We then have\n",
    "\n",
    "$$ \\textit{WeightedSum}((s^{n})_{n \\in \\{1, ..., N\\}})_i = \\sum_{n \\in \\{1, ..., N\\}}{\\lambda_n \\overline{s}_i^n} \n",
    "$$\n",
    "\n",
    "where $\\lambda_n = \\frac{\\max_{j}{s_j^n} - \\min_{j}{s_j^n}}{\\mu(s^n) - \\min_{j}{s_j^n}}$ with $\\mu(s^n)$ the mean of $s^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "qEGZDKM4PyzP",
    "outputId": "7b196571-3ad8-4c51-c386-5fc7af29000c"
   },
   "outputs": [],
   "source": [
    "def min_abs_scaling(array):\n",
    "    return array / abs(np.min(array, axis=0) + 1e-8)\n",
    "\n",
    "\n",
    "def weighted_sum_aggregation(array):\n",
    "    min_array = array.min(axis=0)\n",
    "    weights = (array.max(axis=0) - min_array) / (array.mean(axis=0) - min_array)\n",
    "\n",
    "    return min_abs_scaling(array) @ weights\n",
    "\n",
    "\n",
    "aggregated_scores = weighted_sum_aggregation(scores)\n",
    "\n",
    "# Display scaled and aggregated scores\n",
    "append_scaled_aggregated_scores = np.append(\n",
    "    np.ones(window_size // 2) * float(\"inf\"), aggregated_scores\n",
    ")\n",
    "rpt.display(append_scaled_aggregated_scores, bkps)\n",
    "_ = plt.title(\"Scaled and aggregated score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now detect the changepoints from the newly computed score. For that we need to define a *DummyCost* that will allow us to leverage `ruptures` power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4u4Hx1vtWdw-"
   },
   "outputs": [],
   "source": [
    "from ruptures.base import BaseCost\n",
    "\n",
    "\n",
    "class DummyCost(BaseCost):\n",
    "\n",
    "    r\"\"\"\n",
    "    Dummy cost to pretend a real cost function.\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"Dummy\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the object.\"\"\"\n",
    "        self.signal = None\n",
    "\n",
    "    def fit(self, signal) -> \"DummyCost\":\n",
    "        \"\"\"Set parameters of the instance.\n",
    "        Args:\n",
    "            signal (array): signal. Shape (n_samples,) or (n_samples, n_features)\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        if signal.ndim == 1:\n",
    "            self.signal = signal.reshape(-1, 1)\n",
    "        else:\n",
    "            self.signal = signal\n",
    "\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end) -> float:\n",
    "        \"\"\"Return the approximation cost on the segment [start:end].\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "        Returns:\n",
    "            + infinity\n",
    "        \"\"\"\n",
    "        return float(\"inf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are using the window search method to predict the changepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "Ly9e_TLpQ_PB",
    "outputId": "99f3d70c-3f6b-47f9-fda5-fe544f385e6e"
   },
   "outputs": [],
   "source": [
    "# create the ensemble change point detector\n",
    "dummy_cost = DummyCost().fit(signal)\n",
    "algo = rpt.Window(width=window_size, custom_cost=dummy_cost, jump=1)\n",
    "algo.fit(signal)\n",
    "algo.score = aggregated_scores\n",
    "\n",
    "ensemble_predicted_bkps = algo.predict(n_bkps=n_bkps)\n",
    "rand_indexes[\"ensemble\"] = np.around(randindex(bkps, ensemble_predicted_bkps), 3)\n",
    "\n",
    "_ = [print(cost, rand_index) for cost, rand_index in rand_indexes.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "8bubkcRYQzWf",
    "outputId": "195c0ee5-9f5f-4f17-f3ac-c27a51de6701"
   },
   "outputs": [],
   "source": [
    "_ = rpt.display(signal, bkps, ensemble_predicted_bkps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this example, we have seen how to build an ensemble model to detect changepoints in a few lines of code.\n",
    "\n",
    "This example is using a window search method, the algorithm for ensemble models using other search methods like Binary segmentation or Dynamic programming are given in the paper [Katser2021](#Katser2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "This example notebook has been authored by [Théo VINCENT](https://github.com/theovincent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3JICZPZZ5dU"
   },
   "source": [
    "## References\n",
    "\n",
    "<a id=\"Katser2021\">[Katser2021]</a>\n",
    "Katser, I., Kozitsin, V., Lobachev, V., & Maksimov, I. (2021). Unsupervised Offline Changepoint Detection Ensembles. Applied Sciences, 11(9), 4280."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1ufL03Ine4kl",
    "lFfnkl2rTAsy",
    "uozXK6DbnC5p",
    "eOIc_86NKLBi",
    "Vmhms6jkMjH4"
   ],
   "name": "CPDE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
