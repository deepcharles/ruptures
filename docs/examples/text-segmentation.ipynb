{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear text segmentation\n",
    "\n",
    "<!-- {{ add_binder_block(page) }} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Linear text segmentation consists in dividing a text into several meaningful segments.\n",
    "Linear text segmentation can be seen as a change point detection task and therefore can be carried out with `ruptures`. \n",
    "This example performs exactly that on a well-known data set intoduced in [[Choi2000](#Choi2000)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First we import packages and define a few utility functions.\n",
    "This section can be skipped at first reading.\n",
    "\n",
    "**Library imports.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # For regular expression\n",
    "from textwrap import wrap  # Format text output nicely\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import ruptures as rpt  # our package\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from ruptures.base import BaseCost\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORD_SET = set(\n",
    "    stopwords.words(\"english\")\n",
    ")  # set of stopwords of the English language\n",
    "PUNCTUATION_SET = set(\"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_ax(figsize=(15, 5), dpi=150):\n",
    "    \"\"\"Return a (matplotlib) figure and ax objects with given size.\"\"\"\n",
    "    return plt.subplots(figsize=figsize, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_text(filepath: Path) -> (list, list):\n",
    "    \"\"\"Read a file and return the text and the paragraphs' boundaries.\n",
    "\n",
    "    The text is returned as a list of sentences.\n",
    "    The paragraphs' boundaries are returned as a list of indexes.\n",
    "    \"\"\"\n",
    "    list_of_excerpts = filepath.read_text().strip(\"==========\\n\").split(\"==========\")\n",
    "    true_bkps = list()\n",
    "    original_text = list()\n",
    "    for excerpt in list_of_excerpts:\n",
    "        list_of_sentences = excerpt.strip(\"\\n\").split(\"\\n\")\n",
    "        true_bkps.append(len(list_of_sentences))\n",
    "        original_text.extend(list_of_sentences)\n",
    "    true_bkps = np.cumsum(true_bkps).tolist()\n",
    "\n",
    "    return original_text, true_bkps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(list_of_sentences: list) -> list:\n",
    "    \"\"\"Preprocess each sentence (remove punctuation, stopwords, then stemming.)\"\"\"\n",
    "    transformed = list()\n",
    "    for sentence in list_of_sentences:\n",
    "        ps = PorterStemmer()\n",
    "        list_of_words = regexp_tokenize(text=sentence.lower(), pattern=\"\\w+\")\n",
    "        list_of_words = [\n",
    "            ps.stem(word) for word in list_of_words if word not in STOPWORD_SET\n",
    "        ]\n",
    "        transformed.append(\" \".join(list_of_words))\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The text to segment is a concatenation of excerpts from ten different documents randomly selected from the so-called Brown corpus (described [here](http://icame.uib.no/brown/bcm.html)).\n",
    "Each excerpt has nine to eleven sentences, amounting to 99 sentences in total.\n",
    "The complete text is shown in [Appendix A](#Appendix-A).\n",
    "\n",
    "These data stem from a larger data set which is thoroughly described in [[Choi2000](#Choi2000)] and can be downloaded [here](https://web.archive.org/web/20030206011734/http://www.cs.man.ac.uk/~mary/choif/software/C99-1.2-release.tgz).\n",
    "This is a common benchmark to evaluate text segmentation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the text\n",
    "filepath = Path(\"../data/0.ref\")\n",
    "original_text, true_bkps = load_original_text(filepath=filepath)\n",
    "\n",
    "print(f\"There are {len(original_text)} sentences, from {len(true_bkps)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to automatically recover the boundaries of the 10 excerpts, using the fact that they come from quite different documents and therefore have distinct topics.\n",
    "\n",
    "For instance, in the small extract of text printed in the following cell, an accurate text segmentation procedure would be able to detect that the first two sentences (10 and 11) and the last three sentences (12 to 14) belong to two different documents and have with very different semantic fields.\n",
    "\n",
    "<!--\n",
    "<p align=\"center\">\n",
    "  <img width=\"50%\" src=\"/images/choi_example.png\">\n",
    "</p>\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 5 sentences from the original text\n",
    "start, end = 9, 14\n",
    "for (line_number, sentence) in enumerate(original_text[start:end], start=start + 1):\n",
    "    sentence = sentence.strip(\"\\n\")\n",
    "    print(f\"{line_number:>2}: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "\n",
    "Before performing text segmentation, the original text is preprocessed.\n",
    "In a nutshell (see [[Choi2000](#Choi2000)] for more detail),\n",
    "\n",
    "- the punctuation and stopwords are removed;\n",
    "- words are reduced to their stems (e.g., \"waited\" and \"waiting\" become \"wait\");\n",
    "- a vector of word counts is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text\n",
    "transformed_text = preprocess(original_text)\n",
    "# print original and transformed\n",
    "ind = 97\n",
    "print(\"Original sentence:\")\n",
    "print(f\"\\t{original_text[ind]}\")\n",
    "print()\n",
    "print(\"Transformed:\")\n",
    "print(f\"\\t{transformed_text[ind]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the text is preprocessed, each sentence is transformed into a vector of word counts.\n",
    "vectorizer = CountVectorizer(analyzer=\"word\")\n",
    "vectorized_text = vectorizer.fit_transform(transformed_text)\n",
    "\n",
    "msg = f\"There are {len(vectorizer.get_feature_names())} different words in the corpus, e.g. {vectorizer.get_feature_names()[20:30]}.\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vectorized text representation is a (very) sparse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare (the vectorized representation of) two sentences, [[Choi2000]](#Choi2000) uses the cosine similarity $k_{\\text{cosine}}: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$:\n",
    "\n",
    "$$ k_{\\text{cosine}}(x, y) := \\frac{\\langle x \\mid y \\rangle}{\\|x\\|\\|y\\|} $$\n",
    "\n",
    "where $x$ and $y$ are two $d$-dimensionnal vectors.\n",
    "\n",
    "Text segmnentation now amounts to a kernel change point detection (see LINK for more details).\n",
    "However, this particular kernel is not implemented in `ruptures` therefore we need to create a [custom cost function](../../user-guide/costs/costcustom).\n",
    "(Actually, it is implemented in `ruptures` but the current implementation does not exploit the sparse structure of the vectorized text representation and can therefore be slow.)\n",
    "\n",
    "Let $y=\\{y_0, y_1,\\dots,y_{T-1}\\}$ be a $d$-dimensionnal signal with $T$ samples.\n",
    "Recall that a cost function $c(\\cdot)$ that derives from a kernel $k(\\cdot, \\cdot)$ is such that\n",
    "\n",
    "$$\n",
    "c(y_{a..b}) = \\sum_{t=a}^{b-1} G_{t, t} - \\frac{1}{b-a} \\sum_{a \\leq s < b } \\sum_{a \\leq t < b} G_{s,t}\n",
    "$$\n",
    "\n",
    "where $y_{a..b}$ is the subsignal $\\{y_a, y_{a+1},\\dots,y_{b-1}\\}$ and $G_{st}:=k(y_s, y_t)$.\n",
    "In other words, $(G_{st})_{st}$ is the $T\\times T$ Gram matrix of $y$.\n",
    "Thanks to this formula, we can now implement our custom cost function (named `CosineCost` in the following cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineCost(BaseCost):\n",
    "    \"\"\"Cost derived from the cosine similarity.\"\"\"\n",
    "\n",
    "    # The 2 following attributes must be specified for compatibility.\n",
    "    model = \"custom_cosine\"\n",
    "    min_size = 2\n",
    "\n",
    "    def fit(self, signal):\n",
    "        \"\"\"Set the internal parameter.\"\"\"\n",
    "        self.signal = signal\n",
    "        self.gram = cosine_similarity(signal, dense_output=False)\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end) -> float:\n",
    "        \"\"\"Return the approximation cost on the segment [start:end].\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "        Returns:\n",
    "            segment cost\n",
    "        Raises:\n",
    "            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n",
    "        \"\"\"\n",
    "        if end - start < self.min_size:\n",
    "            raise NotEnoughPoints\n",
    "        sub_gram = self.gram[start:end, start:end]\n",
    "        val = sub_gram.diagonal().sum()\n",
    "        val -= sub_gram.sum() / (end - start)\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the matrix\n",
    "similarities = np.zeros((n_sentences, n_sentences))\n",
    "# Fill the matrix with similarities\n",
    "for i in np.arange(n_sentences):\n",
    "    for j in np.arange(i, n_sentences):\n",
    "        similarities[i, j] = cosine_similarity(preprocessed[i], preprocessed[j])\n",
    "        similarities[j, i] = similarities[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax((4, 4))\n",
    "plt.imshow(-np.log(similarities), cmap=cm.plasma)\n",
    "ax.set_title(\"Cosine similarities matrix\", fontsize=10)\n",
    "ax.set_xlabel(\"Sentence index\", fontsize=8)\n",
    "ax.set_ylabel(\"Sentence index\", fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some artifacts appear around the similarity matrix diagonal where the cosine measure are a bit higher. \n",
    "\n",
    "The similarity matrix seems to be noisy when looking with a naked eye. We will see that this is not an issue for `ruptures` since the cosine similarity approach offers some nice mathematical properties that we describe below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [Dynamic Programming](../../user-guide/detection/dynp) search method since it has the two following nice properties :\n",
    "\n",
    "* It finds **optimal** boundaries\n",
    "* It allows full modularity and can run with a [Custom Cost](../../user-guide/costs/costcustom) (the [KernelCPD](../../user-guide/detection/kernelcpd) search method implemented in C only allows for a pre-implemented list of kernels and those do not support natively a similarity matrix as an input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object and run the algorythm\n",
    "algo = rpt.Dynp(custom_cost=MyCost(), min_size=2, jump=1).fit(similarities)\n",
    "result = algo.predict(n_bkps=n_bkps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)\n",
    "print(bkps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results\n",
    "\n",
    "### Display boundaries within the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_char = 60  # configure line character width\n",
    "\n",
    "# Initialize some counters\n",
    "c_real_bkps_idx = 0\n",
    "c_computed_bkps_icx = 0\n",
    "line_counter = 1\n",
    "\n",
    "for i, sentence in enumerate(original):\n",
    "    if i == bkps[c_real_bkps_idx]:\n",
    "        # Display real boundaries\n",
    "        print(f\"\\n\\t{'='*nb_char}\\n\")\n",
    "        c_real_bkps_idx += 1\n",
    "    if i == result[c_computed_bkps_icx]:\n",
    "        # Display computed boundaries\n",
    "        print(f\"\\t{'*' *nb_char}\")\n",
    "        c_computed_bkps_icx += 1\n",
    "    sentence_wrap = textwrap.wrap(\n",
    "        sentence[:-1], width=nb_char\n",
    "    )  # removes trailing '\\n' for readability purposes\n",
    "    for j, c_sentence_wrap in enumerate(sentence_wrap):\n",
    "        print(f\"{str(line_counter) + '.' if j == 0 else ''}\\t{c_sentence_wrap}\")\n",
    "    line_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display boundaries on the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax((4, 4))  # creates figure\n",
    "previous_bkp = 0\n",
    "\n",
    "for c_bpks in result:\n",
    "    c_bpks -= 1\n",
    "    ax.vlines(\n",
    "        [c_bpks, previous_bkp],\n",
    "        previous_bkp,\n",
    "        c_bpks,\n",
    "        color=\"black\",\n",
    "        linestyles=\"dashed\",\n",
    "        linewidth=0.8,\n",
    "    )\n",
    "    ax.hlines(\n",
    "        [c_bpks, previous_bkp],\n",
    "        previous_bkp,\n",
    "        c_bpks,\n",
    "        color=\"black\",\n",
    "        linestyles=\"dashed\",\n",
    "        linewidth=0.8,\n",
    "    )\n",
    "    previous_bkp = c_bpks\n",
    "plt.imshow(-np.log(similarities), cmap=cm.plasma)\n",
    "ax.set_title(\"Cosine similarities matrix\\nWith computed boudaries\", fontsize=10)\n",
    "ax.set_xlabel(\"Sentence index\", fontsize=8)\n",
    "ax.set_ylabel(\"Sentence index\", fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A\n",
    "\n",
    "The complete text used in this notebook is as follows.\n",
    "Note that the line numbers and the blank lines (added to visually mark the boundaries between excerpts) are not part of the text fed to the segmentation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (start, end) in rpt.utils.pairwise([0] + bkps):\n",
    "    excerpt = original[start:end]\n",
    "    for (n_line, sentence) in enumerate(excerpt, start=start + 1):\n",
    "        sentence = sentence.strip(\"\\n\")\n",
    "        print(f\"{n_line:>2}: {sentence}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"Choi2000\">[Choi2000]</a>\n",
    "Choi, F. Y. Y. (2000). Advances in domain independent linear text segmentation. Proceedings of the North American Chapter of the Association for Computational Linguistics Conference (NAACL), 26–33.\n",
    "\n",
    "<a id=\"ChoiDataset\">[ChoiDataset]</a>\n",
    "The dataset can be obtained from an archived version of the C99 segmentation [code release](http://web.archive.org/web/20010422042459/http://www.cs.man.ac.uk/~choif/software/C99-1.2-release.tgz). We thank [[Alemi & Ginsparg](#Alemi_Ginsparg)] for pointing to the dataset link. \n",
    "\n",
    "<a id=\"BrownCorpus\">[BrownCorpus]</a>\n",
    "Manual accessible [here](http://icame.uib.no/brown/bcm.html), Henry Kučera and W. Nelson Francis, Brown University, Department of Linguistics, 1964, revised 1971, Revised and Amplified 1979\n",
    "\n",
    "<a id=\"Alemi_Ginsparg\">[Alemi & Ginsparg]</a>\n",
    "Alexander A Alemi and Paul Ginsparg, Text Segmentation based on Semantic Word Embeddings, March 15th 2015, Cornell University, accessible [here](https://arxiv.org/pdf/1503.05543.pdf)\n",
    "\n",
    "<a id=\"Porter1980\">[Porter1980]</a>\n",
    "M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130-137, July. We "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank, to be deleted\n",
    "def get_rank(matrix: np.array, vicinity: int):\n",
    "    res = np.zeros(matrix.shape)\n",
    "    n_lines, n_columns = matrix.shape\n",
    "    for i in np.arange(n_lines):\n",
    "        for j in np.arange(i, n_columns):\n",
    "            sub_matrix = matrix[\n",
    "                max(0, i - vicinity) : min(i + vicinity, n_lines),\n",
    "                max(0, j - vicinity) : min(j + vicinity, n_columns),\n",
    "            ]\n",
    "            res[i, j] = np.sum(np.where(sub_matrix < matrix[i, j]))\n",
    "            res[j, i] = res[i, j]\n",
    "    return res\n",
    "\n",
    "\n",
    "similarities_rank = get_rank(similarities, 3)\n",
    "print(similarities_rank.shape)\n",
    "fig, ax = fig_ax((5, 3))\n",
    "plt.imshow(-similarities_rank, cmap=cm.plasma)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = rpt.Pelt(custom_cost=MyCost(), min_size=2, jump=1).fit(similarities)\n",
    "res_n_bkps = []\n",
    "res_sum_of_cost = []\n",
    "x = np.logspace(-0.5, 0.5, num=100)\n",
    "for pen in x:\n",
    "    result = algo.predict(pen=pen)\n",
    "    # print(result)\n",
    "    res_n_bkps.append(len(result) - 1)\n",
    "    res_sum_of_cost.append(algo.cost.sum_of_costs(result))\n",
    "\n",
    "fig, ax = fig_ax((5, 3))\n",
    "ax.plot(x, res_n_bkps, \"b-\")\n",
    "ax.set_xlabel(\"penality\")\n",
    "ax.set_ylabel(\"Number of computed break points\", color=\"b\")\n",
    "ax.vlines(x[58], 0, 50, colors=\"red\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x, res_sum_of_cost, \"r.\")\n",
    "ax2.set_ylabel(\"Sum of costs\", color=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = rpt.Dynp(custom_cost=MyCost(), min_size=2, jump=1).fit(similarities)\n",
    "\n",
    "\n",
    "def get_sum_of_cost(algo, n_bkps) -> float:\n",
    "    \"\"\"Return the sum of costs for the change points `bkps`\"\"\"\n",
    "    bkps = algo.predict(n_bkps=n_bkps)\n",
    "    return algo.cost.sum_of_costs(bkps)\n",
    "\n",
    "\n",
    "n_bkps_max = 20  # K_max\n",
    "array_of_n_bkps = np.arange(1, n_bkps_max + 1)\n",
    "\n",
    "fig, ax = fig_ax((5, 3))\n",
    "ax.plot(\n",
    "    array_of_n_bkps,\n",
    "    [get_sum_of_cost(algo=algo, n_bkps=i) for i in array_of_n_bkps],\n",
    "    \"-*\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "ax.set_xticks(array_of_n_bkps)\n",
    "ax.set_xlabel(\"Number of change points\")\n",
    "ax.set_title(\"Sum of costs\")\n",
    "ax.grid(axis=\"x\")\n",
    "ax.set_xlim(0, n_bkps_max + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
