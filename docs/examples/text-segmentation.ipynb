{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear text segmentation\n",
    "\n",
    "<!-- {{ add_binder_block(page) }} -->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Linear text segmentation is a Natural Language Processing task consisting in dividing a written text into several meaningful segments. Linear text segmentation can be seen as a change point detection task and therefore can be carried out with `ruptures`. \n",
    "\n",
    "For our current setup, the atomic unit is the sentence: we are searching for the best set of boundaries for which all sentences between two consecutive boundaries are in the same meaningful segment. We replicate the methodology introduced in [[Choi2000](#Choi2000)]. \n",
    "\n",
    "### Data\n",
    "\n",
    "We use the [[Choi Dataset](#ChoiDataset)]. It is a widely used reference dataset to test whether a segmentation algorithm can separate a written text into several meaningfull topic units. Each data file concatenates the first `n` sentences from ten different documents chosen at random from a 124 documents subset of the [[Brown Corpus](#BrownCorpus)] (the ca\\**.pos and cj\\**.pos sets). \n",
    "\n",
    "In order to evaluate the true power of the Linear Text Segmentation algorithm, `n` is randomly chosen within a range and there are several option offered by the [[Choi Dataset](#ChoiDataset)]. Here is an example of what the data might look like for $n \\in [3–5]$\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/choi_example.png\" alt=\"Choi Dataset Snapshot\" width=\"50%\"/>\n",
    "</center>\n",
    "\n",
    "In the current example, we choose $n \\in [9–11]$, meaning the dataset will be the concatenation of 9 to 11 consecutives sentences from ten different documents. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, we make the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import join  # to access the datafile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import ruptures as rpt  # our package\n",
    "from ruptures.base import BaseCost  # To create custom cost\n",
    "import textwrap  # Format text output nicely\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import string  # For the ponctuation\n",
    "import re  # For regular expression, especially to remove the \"'s\" pattern\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    ")  # For counting word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_ax(figsize=(15, 5), dpi=150):\n",
    "    \"\"\"Return a (matplotlib) figure and ax objects with given size.\"\"\"\n",
    "    return plt.subplots(figsize=figsize, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preproccess data\n",
    "\n",
    "For the preproccessing, we follow the steps described in [[Choi2000](#Choi2000)] section `3.1`, i.e. :\n",
    "\n",
    "* Remove punctuation and stopwords : `Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern matcher and a stopword list`,\n",
    "* Tokenize words,\n",
    "* Stem remaining tokens : `A stemming algorithm (Porter, 1980) is then applied to the remaining tokens to obtain the word stems`. We also use the [[Porter1980](#Porter1980)] algorythm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datafile(filename: str):\n",
    "    \"\"\"Read the data and apply the preprocessing\n",
    "    Args:\n",
    "        filename (str): full path the data file\n",
    "\n",
    "    Returns:\n",
    "        original (list): list of sentences, original data without preprocessing applied\n",
    "        bkps (list): sorted list of breakpoints\n",
    "        preprocessed (list): list of sentences, once preprocessing has been applied\n",
    "    \"\"\"\n",
    "    original = []\n",
    "    preprocessed = []\n",
    "    bkps = []\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            # Handle ground truth boundaries\n",
    "            if line == \"==========\\n\":\n",
    "                # Skip the first boundary in file header\n",
    "                if len(original) > 0:\n",
    "                    bkps.append(i - 1 - len(bkps))\n",
    "                continue\n",
    "            else:\n",
    "                original.append(line)\n",
    "\n",
    "                # Apply preprocessing\n",
    "                line = re.sub(\"'s\", \"\", line)  # remove \"'s\" strings\n",
    "                line = line.translate(\n",
    "                    str.maketrans(\"\", \"\", string.punctuation)\n",
    "                )  # removes punctuation\n",
    "                text_tokens = word_tokenize(line)  # tokenize\n",
    "                ps = PorterStemmer()  # stem\n",
    "                text_tokens = [ps.stem(word) for word in text_tokens]\n",
    "                tokens_without_sw = [\n",
    "                    word for word in text_tokens if not word in stopwords.words()\n",
    "                ]  # removes stopwords\n",
    "\n",
    "                # Append new preprocessed sentence\n",
    "                preprocessed.append(tokens_without_sw)\n",
    "        return original, bkps, preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original, bkps, preprocessed = read_datafile(join(\"../data/\", \"0.ref\"))\n",
    "n_sentences = len(preprocessed)\n",
    "n_bkps = len(bkps) - 1\n",
    "\n",
    "print(f\"There are {n_sentences} sentences to be seperated into {n_bkps + 1} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find text segment boundaries\n",
    "\n",
    "### Define similarity measure\n",
    "\n",
    "As in [[Choi2000](#Choi2000)] section `3.1`, we use the cosine similarity between two sentences. \n",
    "\n",
    "$$ sim(x, y) = \\frac{\\sum_w f_{x, w} \\times f_{y, w}}{\\sqrt{\\sum_w f_{x, w}^{2} \\times \\sum_w f_{y, w}^{2}}} $$\n",
    "\n",
    "Where : \n",
    "\n",
    "* $f_{x, w}$ denotes the frequency of word $w$ in sentence $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(s1: str, s2: str):\n",
    "    \"\"\"Compute cosine similarity given two sentences\n",
    "    Args:\n",
    "        s1 (str): first sentence\n",
    "        s2 (str): second sentence\n",
    "\n",
    "    Returns:\n",
    "        float: the cosine distance between s1 and s2\n",
    "    \"\"\"\n",
    "    corpus = [\n",
    "        \" \".join(s1),\n",
    "        \" \".join(s2),\n",
    "    ]\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus).toarray()\n",
    "    cosine = np.sum(X[0, :] * X[1, :])\n",
    "    cosine = cosine / (np.sum(X[0, :] * X[0, :]) * np.sum(X[1, :] * X[1, :])) ** 0.5\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the matrix\n",
    "similarities = np.zeros((n_sentences, n_sentences))\n",
    "# Fill the matrix with similarities\n",
    "for i in np.arange(n_sentences):\n",
    "    for j in np.arange(i, n_sentences):\n",
    "        similarities[i, j] = cosine_similarity(preprocessed[i], preprocessed[j])\n",
    "        similarities[j, i] = similarities[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax((5, 5))\n",
    "plt.imshow(-np.log(similarities), cmap=cm.plasma)\n",
    "ax.set_title(\"Cosine similarities matrix\", fontsize=10)\n",
    "ax.set_xlabel(\"Sentence index\", fontsize=8)\n",
    "ax.set_ylabel(\"Sentence index\", fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some artifacts appear around the similarity matrix diagonal where the cosine measure are a bit higher. \n",
    "\n",
    "The similarity matrix seems to be noisy when looking with a naked eye. We will see that this is not an issue for `ruptures` since the cosine similarity approach offers some nice mathematical properties that we describe below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search best boudaries\n",
    "\n",
    "Since the cosine similarity is a positive semi-definite kernel $k(\\cdot, \\cdot) : \\mathbb{R}^d\\times \\mathbb{R}^d \\mapsto \\mathbb{R}$, with in our case \n",
    "\n",
    "$$ k(x, y) = \\frac{\\langle x\\mid y\\rangle}{|x||y|} = sim(x, y) $$\n",
    "\n",
    "Where $\\langle \\cdot\\mid\\cdot \\rangle$ and $| \\cdot |$ are the Euclidean scalar product and norm respectively.\n",
    "\n",
    "We can then write the cost function as follows :\n",
    "\n",
    "$$ c(y_{a..b}) = D_{a..b} - \\frac{1}{b-a} S_{a..b, a..b} $$\n",
    "\n",
    "With :\n",
    "\n",
    "* $y_{a..b}$: written text with sentences with indexes within $[a, b[$\n",
    "* $c(y_{a..b})$: cost on segment $y_{a..b}$\n",
    "* $D_{a..b} = \\sum_{t=a}^{b-1} k(y_t, y_t)$\n",
    "* $S_{a..b, a'..b'} = \\sum_{a \\leq s < b } \\sum_{a' \\leq t < b'} k(y_s, y_t)$\n",
    "* $D_{a..a} = 0$\n",
    "* $S_{a..a, a'..b'} = S_{a..b, a'..a'} = 0$ \n",
    "\n",
    "Therefore, based on the computed similarity matrix (which is a Gram matrix), we create a [Custom Cost](../../user-guide/costs/costcustom) as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCost(BaseCost):\n",
    "    \"\"\"Custom cost for gram matrix.\"\"\"\n",
    "\n",
    "    # The 2 following attributes must be specified for compatibility.\n",
    "    model = \"\"\n",
    "    min_size = 2\n",
    "\n",
    "    def fit(self, gram_matrix):\n",
    "        \"\"\"Set the internal parameter.\"\"\"\n",
    "        self.signal = gram_matrix\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end) -> float:\n",
    "        \"\"\"Return the approximation cost on the segment [start:end].\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "        Returns:\n",
    "            segment cost\n",
    "        Raises:\n",
    "            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n",
    "        \"\"\"\n",
    "        if end - start < self.min_size:\n",
    "            raise NotEnoughPoints\n",
    "        sub_gram = self.signal[start:end, start:end]\n",
    "        val = np.diagonal(sub_gram).sum()\n",
    "        val -= sub_gram.sum() / (end - start)\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = rpt.Dynp(custom_cost=MyCost(), min_size=2, jump=1).fit(similarities)\n",
    "result = algo.predict(n_bkps=n_bkps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)\n",
    "print(bkps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results\n",
    "\n",
    "### Display boundaries within the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_real_bkps_idx = 0\n",
    "c_computed_bkps_icx = 0\n",
    "line_counter = 1\n",
    "nb_char = 60\n",
    "for i, sentence in enumerate(original):\n",
    "    if i == bkps[c_real_bkps_idx]:\n",
    "        print(f\"\\n\\t{'='*nb_char}\\n\")\n",
    "        c_real_bkps_idx += 1\n",
    "    if i == result[c_computed_bkps_icx]:\n",
    "        print(f\"\\t{'*' *nb_char}\")\n",
    "        c_computed_bkps_icx += 1\n",
    "    sentence_wrap = textwrap.wrap(\n",
    "        sentence[:-1], width=nb_char\n",
    "    )  # removes trailing '\\n' for readability purposes\n",
    "    for j, c_sentence_wrap in enumerate(sentence_wrap):\n",
    "        print(f\"{str(line_counter) + '.' if j == 0 else ''}\\t{c_sentence_wrap}\")\n",
    "    line_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display boundaries on the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax((5, 5))\n",
    "boundaries_width = 0.8\n",
    "previous_bkp = 0\n",
    "linestyle = \"dashed\"\n",
    "color = \"black\"\n",
    "for c_bpks in result:\n",
    "    c_bpks -= 1\n",
    "    ax.vlines(\n",
    "        [c_bpks, previous_bkp],\n",
    "        previous_bkp,\n",
    "        c_bpks,\n",
    "        color=color,\n",
    "        linestyles=linestyle,\n",
    "        linewidth=boundaries_width,\n",
    "    )\n",
    "    ax.hlines(\n",
    "        [c_bpks, previous_bkp],\n",
    "        previous_bkp,\n",
    "        c_bpks,\n",
    "        color=color,\n",
    "        linestyles=linestyle,\n",
    "        linewidth=boundaries_width,\n",
    "    )\n",
    "    previous_bkp = c_bpks\n",
    "plt.imshow(-np.log(similarities), cmap=cm.plasma)\n",
    "ax.set_title(\"Cosine similarities matrix\\nWith computed boudaries\", fontsize=10)\n",
    "ax.set_xlabel(\"Sentence index\", fontsize=8)\n",
    "ax.set_ylabel(\"Sentence index\", fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "<a id=\"Choi2000\">[Choi2000]</a>\n",
    "Freddy Y. Y. Choi , NAACL 2000: Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, April 2000, Pages 26–33, accessible [here](https://www.aclweb.org/anthology/A00-2004.pdf)\n",
    "\n",
    "<a id=\"ChoiDataset\">[ChoiDataset]</a>\n",
    "The dataset can be obtained from an archived version of the C99 segmentation [code release](http://web.archive.org/web/20010422042459/http://www.cs.man.ac.uk/~choif/software/C99-1.2-release.tgz). We thank [[Alemi & Ginsparg](#Alemi_Ginsparg)] for pointing to the dataset link. \n",
    "\n",
    "<a id=\"BrownCorpus\">[BrownCorpus]</a>\n",
    "Manual accessible [here](http://icame.uib.no/brown/bcm.html), Henry Kučera and W. Nelson Francis, Brown University, Department of Linguistics, 1964, revised 1971, Revised and Amplified 1979\n",
    "\n",
    "<a id=\"Alemi_Ginsparg\">[Alemi & Ginsparg]</a>\n",
    "Alexander A Alemi and Paul Ginsparg, Text Segmentation based on Semantic Word Embeddings, March 15th 2015, Cornell University, accessible [here](https://arxiv.org/pdf/1503.05543.pdf)\n",
    "\n",
    "<a id=\"Porter1980\">[Porter1980]</a>\n",
    "M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130-137, July. We "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank, to be deleted\n",
    "def get_rank(matrix: np.array, vicinity: int):\n",
    "    res = np.zeros(matrix.shape)\n",
    "    n_lines, n_columns = matrix.shape\n",
    "    for i in np.arange(n_lines):\n",
    "        for j in np.arange(i, n_columns):\n",
    "            sub_matrix = matrix[\n",
    "                max(0, i - vicinity) : min(i + vicinity, n_lines),\n",
    "                max(0, j - vicinity) : min(j + vicinity, n_columns),\n",
    "            ]\n",
    "            res[i, j] = np.sum(np.where(sub_matrix < matrix[i, j]))\n",
    "            res[j, i] = res[i, j]\n",
    "    return res\n",
    "\n",
    "\n",
    "similarities_rank = get_rank(similarities, 3)\n",
    "print(similarities_rank.shape)\n",
    "fig, ax = fig_ax((5, 3))\n",
    "plt.imshow(-similarities_rank, cmap=cm.plasma)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = rpt.Pelt(custom_cost=MyCost(), min_size=2, jump=1).fit(similarities)\n",
    "res_n_bkps = []\n",
    "res_sum_of_cost = []\n",
    "x = np.logspace(-0.5, 0.5, num=100)\n",
    "for pen in x:\n",
    "    result = algo.predict(pen=pen)\n",
    "    # print(result)\n",
    "    res_n_bkps.append(len(result) - 1)\n",
    "    res_sum_of_cost.append(algo.cost.sum_of_costs(result))\n",
    "\n",
    "fig, ax = fig_ax((5, 3))\n",
    "ax.plot(x, res_n_bkps, \"b-\")\n",
    "ax.set_xlabel(\"penality\")\n",
    "ax.set_ylabel(\"Number of computed break points\", color=\"b\")\n",
    "ax.vlines(x[58], 0, 50, colors=\"red\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x, res_sum_of_cost, \"r.\")\n",
    "ax2.set_ylabel(\"Sum of costs\", color=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = rpt.Dynp(custom_cost=MyCost(), min_size=2, jump=1).fit(similarities)\n",
    "\n",
    "\n",
    "def get_sum_of_cost(algo, n_bkps) -> float:\n",
    "    \"\"\"Return the sum of costs for the change points `bkps`\"\"\"\n",
    "    bkps = algo.predict(n_bkps=n_bkps)\n",
    "    return algo.cost.sum_of_costs(bkps)\n",
    "\n",
    "\n",
    "n_bkps_max = 20  # K_max\n",
    "array_of_n_bkps = np.arange(1, n_bkps_max + 1)\n",
    "\n",
    "fig, ax = fig_ax((5, 3))\n",
    "ax.plot(\n",
    "    array_of_n_bkps,\n",
    "    [get_sum_of_cost(algo=algo, n_bkps=i) for i in array_of_n_bkps],\n",
    "    \"-*\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "ax.set_xticks(array_of_n_bkps)\n",
    "ax.set_xlabel(\"Number of change points\")\n",
    "ax.set_title(\"Sum of costs\")\n",
    "ax.grid(axis=\"x\")\n",
    "ax.set_xlim(0, n_bkps_max + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ruptures",
   "language": "python",
   "name": "ruptures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
