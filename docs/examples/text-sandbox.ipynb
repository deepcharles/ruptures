{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import ruptures as rpt  # our package\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/BoiseState/CS121-Public/master/projects/p4-readerofbooks/etext/Moby-Dick-Broken.txt\"\n",
    "url = \"https://sherlock-holm.es/stories/plain-text/houn.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "# retrieving data from the URL using get method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = str(r.content)\n",
    "chapter1 = content[content.find(\"CHAPTER I\") : content.find(\"CHAPTER II\")]\n",
    "chapter2 = content[content.find(\"CHAPTER II\") : content.find(\"CHAPTER III\")]\n",
    "\n",
    "# content = content[content.find(\"Original Transcriber's Notes:\"):]\n",
    "# chapter1 = content[content.find(\"CHAPTER 1. Loomings.\"):content.find(\"CHAPTER 2. The Carpet-Bag.\")]\n",
    "# chapter2 = content[content.find(\"CHAPTER 2. The Carpet-Bag.\"):content.find(\"CHAPTER 3. The Spouter-Inn.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chapter1.split(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter1_sentences = chapter1.split(\".\")\n",
    "print(len(chapter1_sentences))\n",
    "\n",
    "# Remove chapter section and chapter title, replace it by empy string\n",
    "chapter1_sentences[0] = \"\"\n",
    "chapter1_sentences[1] = \"\"\n",
    "\n",
    "for i, sentence in enumerate(chapter1_sentences):\n",
    "    chapter1_sentences[i] = re.sub(\"\\\\\\\\n\", \"\", sentence)\n",
    "    chapter1_sentences[i] = chapter1_sentences[i].lower()\n",
    "    chapter1_sentences[i] = re.sub(\n",
    "        r\"\"\"([!\"#$%&\\'()*+,-./:;<=>?@[\\\\\\]^_`{|}~])\"\"\", \" \", chapter1_sentences[i]\n",
    "    )  # Non ponctuation\n",
    "    chapter1_sentences[i] = re.sub(\n",
    "        r\"([^A-Za-z ])\", \" \", chapter1_sentences[i]\n",
    "    )  # non letters\n",
    "    chapter1_sentences[i] = re.sub(\n",
    "        \"\\s+\", \" \", chapter1_sentences[i]\n",
    "    )  # several spaces -> single space\n",
    "    if len(chapter1_sentences[i]) < 3:\n",
    "        chapter1_sentences[i] = \"\"\n",
    "\n",
    "print(len(chapter1_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter1_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter2_sentences = chapter2.split(\".\")\n",
    "print(len(chapter2_sentences))\n",
    "\n",
    "# Remove chapter section and chapter title, replace it by empy string\n",
    "chapter2_sentences[0] = \"\"\n",
    "chapter2_sentences[1] = \"\"\n",
    "\n",
    "for i, sentence in enumerate(chapter2_sentences):\n",
    "    chapter2_sentences[i] = re.sub(\"\\\\\\\\n\", \"\", sentence)\n",
    "    chapter2_sentences[i] = chapter2_sentences[i].lower()\n",
    "    chapter2_sentences[i] = re.sub(\n",
    "        r\"\"\"([!\"#$%&\\'()*+,-./:;<=>?@[\\\\\\]^_`{|}~])\"\"\", \" \", chapter2_sentences[i]\n",
    "    )  # Non ponctuation\n",
    "    chapter2_sentences[i] = re.sub(\n",
    "        r\"([^A-Za-z ])\", \" \", chapter2_sentences[i]\n",
    "    )  # non letters\n",
    "    chapter2_sentences[i] = re.sub(\n",
    "        \"\\s+\", \" \", chapter2_sentences[i]\n",
    "    )  # several spaces -> single space\n",
    "    if len(chapter2_sentences[i]) < 3:\n",
    "        chapter2_sentences[i] = \"\"\n",
    "\n",
    "print(len(chapter2_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter2_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter1_sentences = chapter1_sentences + chapter2_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chapter1_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chapter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter1 = chapter1 + chapter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "chapter1_sentences = chapter1.split(\".\")\n",
    "for idx, sentence in enumerate(chapter1_sentences):\n",
    "    chapter1_sentences[idx] = simple_preprocess(sentence)\n",
    "\n",
    "emb_dims = 50\n",
    "word2vec = Word2Vec(sentences=chapter1_sentences, min_count=2, size=emb_dims, window=10)\n",
    "word2vec.train(\n",
    "    chapter1_sentences, total_examples=word2vec.corpus_count, epochs=word2vec.epochs\n",
    ")  # train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings_mean(word2vec_model, sentence):\n",
    "    mean = np.zeros((1, emb_dims))\n",
    "    vocab_keys = word2vec.wv.vocab.keys()\n",
    "    in_vocab = [el for el in sentence if el in vocab_keys]\n",
    "    if len(in_vocab) == 0:\n",
    "        return np.full((1, emb_dims), np.nan)\n",
    "    for el in in_vocab:\n",
    "        mean = mean + word2vec.wv[el].reshape(1, -1)\n",
    "    return mean / len(in_vocab)\n",
    "\n",
    "\n",
    "get_embeddings_mean(word2vec, chapter1_sentences[7])\n",
    "\n",
    "X = np.zeros((len(chapter1_sentences), emb_dims))\n",
    "for idx, sentence in enumerate(chapter1_sentences):\n",
    "    X[idx, :] = get_embeddings_mean(word2vec, sentence)\n",
    "\n",
    "# np.c_[word2vec.wv['before'], word2vec.wv['visitor']].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.pcolor(X.T, cmap=\"RdBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sentence in enumerate(chapter1_sentences):\n",
    "    print(\n",
    "        np.array(\n",
    "            [word2vec.wv[el] for el in sentence if el in word2vec.wv.vocab.keys()]\n",
    "        ).shape\n",
    "    )\n",
    "    chapter1_sentences[idx] = np.mean(\n",
    "        np.array([word2vec.wv[el] for el in sentence if el in word2vec.wv.vocab.keys()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter1_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html\n",
    "word2vec = Word2Vec(min_count=2)\n",
    "word2vec.build_vocab(chapter1_sentences)  # prepare the model vocabulary\n",
    "word2vec.train(\n",
    "    chapter1_sentences, total_examples=word2vec.corpus_count, epochs=word2vec.epochs\n",
    ")  # train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "\n",
    "model = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
    "model.wv.vocab\n",
    "print(common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents=\"unicode\")\n",
    "X = vectorizer.fit_transform(chapter1_sentences)\n",
    "print(X.shape)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = rpt.Dynp(model=\"l2\", min_size=1, jump=1).fit(X.toarray())\n",
    "result = algo.predict(n_bkps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text_segments(txt, bkps):\n",
    "    txt_sentences = txt.split(\".\")\n",
    "    original = re.sub(\"\\\\\\\\n\", \"\\n\", txt).split(\".\")\n",
    "    res_str = \"\"\n",
    "    for i in range(len(txt_sentences)):\n",
    "        if i in bkps:\n",
    "            res_str = (\n",
    "                res_str\n",
    "                + \"\\n---------------------------------------------------------------------------\\n\"\n",
    "            )\n",
    "        res_str = res_str + original[i] + \".\"\n",
    "\n",
    "    print(res_str)\n",
    "\n",
    "\n",
    "print_text_segments(chapter1, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ruptures",
   "language": "python",
   "name": "ruptures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
